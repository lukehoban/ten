Linear[n,k](x: {...n}) -> {...k}
Ones[...] -> {...}
Sqrt(x: {...}) -> {...}
Inf -> {}

MultiheadAttention[H, M, K, V, Dropout=0.1](x : {bMhk})


CausalSelfAttention[Embed, Heads, BlockSize, dropout=0.1]<B, T>(x : {B T Embed}) -> {B T Embed}:
    Attn = Linear[Embed, Embed*3]
    Proj = Linear[Embed, Embed]
    q,k,v {3BHTK} = Attn(x) {BH(3TK)}


# Single letter uppercase are runtime generic tensor dimension parameters
# PascalCase variables are compile-time generic tensro dimension parameters
# camelCase variables are compile-time 

# Embed, Heads, BlockSize are compile-time generic tensor dimension inputs
# B, T are runtime-generic tensor dimension parameters
# dropout is a compile-time generic parameters
# * Is there any need to differentiate tensor and non-tensor dimension inuts?
# * What computation is allowed on 


CausalSelfAttention[Embed, Heads, dropout=0.1](x : {B T Embed}) -> {B T Embed}:
    q,k,v = Linear[Embed, Embed*3](x) {B T (3 Heads K) -> 3 B Heads T K}
    att = @{BHIJ}(q{BHIK}, k{BHJK}) / Sqrt(Embed/Heads)
    mask = [i,j => i<=j] {IJ->11IJ}
    masked = att[mask ?? -Inf]
    averaged = Dropout[dropout](Softmax[K](masked{BHIK}))
    y = @{BT(HK)}(att{BHTJ}, v{BHJK})
    Dropout["resid_dropout", dropout](Linear["proj", Embed, Embed](y))


CausalSelfAttention[Embed, Heads, dropout=0.1](x : {B T Embed}) -> {B T Embed}:
    q,k,v = Linear[Embed, Embed*3](x) {B T (3 Heads K) -> 3 B Heads T K}
    att = @{BHIJ}(q{BHIK}, k{BHJK}) / Sqrt(Embed/Heads)
    mask = [i,j => i<=j] {IJ->11IJ}
    masked = att[mask ?? -Inf]
    averaged = Dropout[dropout](Softmax[K](masked{BHIK}))
    y = @{BT(HK)}(att{BHTJ}, v{BHJK})
    Dropout["resid_dropout", dropout](Linear["proj", Embed, Embed](y))



CausalSelfAttention[Embed, Heads, dropout](x : {B T Embed}) -> {B T Embed}:
    q,k,v = Linear[Embed, Embed*3](x) {B T (3 Heads K) -> 3 B Heads T K}
    att = @{BHIJ}(q{BHIK}, k{BHJK}) / Sqrt(Embed/Heads)
    mask = [i,j => i<=j] {IJ->11IJ}
    masked = att[mask ?? -Inf]
    averaged = Dropout[dropout](Softmax[K](masked{BHIK}))
    y = @{BT(HK)}(att{BHTJ}, v{BHJK})
    Dropout["resid_dropout", dropout](Linear["proj", Embed, Embed](y))




    


